{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import random\n",
    "import tempfile\n",
    "import unittest\n",
    "import traceback\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import copy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import gc\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "from convlstm import *\n",
    "use_cuda = True\n",
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "fold = 'fold_0'\n",
    "frame_len = 79\n",
    "\n",
    "model_name = 'FacialCueNet(AU,sym,gaze,ME)'\n",
    "train_dataset_name = 'CourtroomDB'\n",
    "test_dataset_name = 'CourtroomDB(fold_0)'\n",
    "\n",
    "# val_fold_dir is str !!! (EX)'fold_0'\n",
    "def fold_to_trainset(fold_path, val_fold_dir):\n",
    "    extracted_path_list = []\n",
    "    au_path_list = []\n",
    "    symmetry_path_list = []\n",
    "    gaze_path_list = []\n",
    "    ME_path_list = []\n",
    "    for fold in sorted(os.listdir(fold_path)):\n",
    "        if fold != val_fold_dir:\n",
    "            for feature in sorted(os.listdir(fold_path+'/'+fold)):\n",
    "                if feature == 'DeceptionDB_feature':\n",
    "                    extracted_feature_path = fold+'/'+feature\n",
    "                    extracted_path_list.append(extracted_feature_path)\n",
    "                elif feature == 'ActionUnit(FPS15)':\n",
    "                    au_feature_path = fold+'/'+feature\n",
    "                    au_path_list.append(au_feature_path)    \n",
    "                elif feature == 'NormalizedCrossCovariance(FPS15)':\n",
    "                    symmetry_feature_path = fold+'/'+feature\n",
    "                    symmetry_path_list.append(symmetry_feature_path) \n",
    "                elif feature == 'gaze_features(FPS15)':\n",
    "                    gaze_feature_path = fold+'/'+feature\n",
    "                    gaze_path_list.append(gaze_feature_path) \n",
    "                elif feature == 'MicroExpression(FPS15)':\n",
    "                    ME_feature_path = fold+'/'+feature\n",
    "                    ME_path_list.append(ME_feature_path) \n",
    "    return extracted_path_list, au_path_list, symmetry_path_list, gaze_path_list, ME_path_list\n",
    "\n",
    "\n",
    "def data_to_dict_test(data_name):\n",
    "    video_name_list = []\n",
    "    class_dict = {}\n",
    "    path = '/Real-life_Deception_Detection_2016/pruned_10fold/{0}/DeceptionDB_feature'.format(fold)\n",
    "    for video in sorted(os.listdir(path)):\n",
    "        video_name = video\n",
    "        if video.split('_')[1] == 'truth':\n",
    "            video_label = 1\n",
    "        elif video.split('_')[1] == 'lie':\n",
    "            video_label = 0\n",
    "        else:\n",
    "            print('Wrong video label was selected !')\n",
    "        video_name_list.append(video_name)\n",
    "        class_dict[video_name] = video_label\n",
    "    \n",
    "    au_file_list = []\n",
    "    au_path = '/Real-life_Deception_Detection_2016/pruned_10fold/{0}/ActionUnit(FPS15)'.format(fold)\n",
    "    for au_file in sorted(os.listdir(au_path)):\n",
    "        au_file_list.append(au_file)\n",
    "        \n",
    "    symmetry_file_list = []\n",
    "    symmetry_path = '/Real-life_Deception_Detection_2016/pruned_10fold/{0}/NormalizedCrossCovariance(FPS15)'.format(fold)\n",
    "    for symmetry_file in sorted(os.listdir(symmetry_path)):\n",
    "        symmetry_file_list.append(symmetry_file)\n",
    "        \n",
    "    gaze_file_list = []\n",
    "    gaze_path = '/Real-life_Deception_Detection_2016/pruned_10fold/{0}/gaze_features(FPS15)'.format(fold)\n",
    "    for gaze_file in sorted(os.listdir(gaze_path)):\n",
    "        gaze_file_list.append(gaze_file)\n",
    "        \n",
    "    ME_file_list = []\n",
    "    ME_path = '/Real-life_Deception_Detection_2016/pruned_10fold/{0}/MicroExpression(FPS15)'.format(fold)\n",
    "    for ME_file in sorted(os.listdir(ME_path)):\n",
    "        ME_file_list.append(ME_file)\n",
    "\n",
    "    return path, class_dict, video_name_list, au_path, au_file_list, symmetry_path, symmetry_file_list, gaze_path, gaze_file_list, ME_path, ME_file_list\n",
    "\n",
    "def data_to_dict_train(data_name):\n",
    "    video_name_list = []\n",
    "    class_dict = {}\n",
    "    path = '/Real-life_Deception_Detection_2016/pruned_10fold'\n",
    "    video_path, au_path, symmetry_path, gaze_path, ME_path = fold_to_trainset(path, fold)\n",
    "    for v_path in video_path:\n",
    "        for video in sorted(os.listdir(path+'/'+v_path)):\n",
    "            video_name = video.split('/')[-1]\n",
    "            if video_name.split('_')[1] == 'truth':\n",
    "                video_label = 1\n",
    "            elif video_name.split('_')[1] == 'lie':\n",
    "                video_label = 0\n",
    "            else:\n",
    "                print('Wrong video label was selected !')\n",
    "            video_name_list.append(v_path+'/'+video)\n",
    "            class_dict[v_path+'/'+video] = video_label\n",
    "    \n",
    "    au_file_list = []\n",
    "    for a_path in au_path:\n",
    "        for au_file in sorted(os.listdir(path+'/'+a_path)):\n",
    "            au_file_list.append(a_path+'/'+au_file)\n",
    "    au_path = path\n",
    "        \n",
    "    symmetry_file_list = []\n",
    "    for s_path in symmetry_path:\n",
    "        for symmetry_file in sorted(os.listdir(path+'/'+s_path)):\n",
    "            symmetry_file_list.append(s_path+'/'+symmetry_file)\n",
    "    symmetry_path = path\n",
    "    \n",
    "    gaze_file_list = []\n",
    "    for g_path in gaze_path:\n",
    "        for gaze_file in sorted(os.listdir(path+'/'+g_path)):\n",
    "            gaze_file_list.append(g_path+'/'+gaze_file)\n",
    "            \n",
    "    gaze_path = path\n",
    "        \n",
    "    ME_file_list = []\n",
    "    for M_path in ME_path:\n",
    "        for ME_file in sorted(os.listdir(path+'/'+M_path)):\n",
    "            ME_file_list.append(M_path+'/'+ME_file)\n",
    "            \n",
    "    ME_path = path\n",
    "\n",
    "    return path, class_dict, video_name_list, au_path, au_file_list, symmetry_path, symmetry_file_list, gaze_path, gaze_file_list, ME_path, ME_file_list\n",
    "\n",
    "class DeceptionDB(Dataset):\n",
    "    def __init__(self, category_dict, data_dir, video_names, actionunit_dirs, actionunit_files, symmetry_dirs, symmetry_files, gaze_dirs, gaze_files, ME_dirs, ME_files, transform=None):\n",
    "        self.data_dir = data_dir # path just before clip npy file\n",
    "        self.video_names = video_names # list of clip name ( npy file name  )\n",
    "        self.category_dict = category_dict # class_dict[video_name]=class\n",
    "        self.AU_dir = actionunit_dirs\n",
    "        self.AU_file_names = actionunit_files\n",
    "        self.symmetry_dir = symmetry_dirs\n",
    "        self.symmetry_file_names = symmetry_files\n",
    "        self.gaze_dir = gaze_dirs\n",
    "        self.gaze_file_names = gaze_files\n",
    "        self.ME_dir = ME_dirs\n",
    "        self.ME_file_names = ME_files\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(self.data_dir, list):\n",
    "            feature_file = os.path.join(self.data_dir[idx],  self.video_names[idx])\n",
    "        else:\n",
    "            feature_file = os.path.join(self.data_dir,  self.video_names[idx])\n",
    "       \n",
    "        video_name = self.video_names[idx]\n",
    "        \n",
    "        feature_per_video = np.load(feature_file)\n",
    "        \n",
    "        label_per_video = np.expand_dims(int(self.category_dict[video_name]), axis=0)  # shape(1,n)\n",
    "    \n",
    "        \n",
    "        if isinstance(self.AU_dir, list):\n",
    "            au_file = os.path.join(self.AU_dir[idx],  self.AU_file_names[idx])\n",
    "        else:\n",
    "            au_file = os.path.join(self.AU_dir,  self.AU_file_names[idx])\n",
    "        \n",
    "        au_df = pd.read_csv(au_file,header=None,index_col=False)\n",
    "        au_array = au_df.values\n",
    "        real_use_AU = [0,1,2,4] # 0:AU15, 1:AU17, 2:AU20, 3:AU25, 4:AU45\n",
    "        au_per_video = au_array[real_use_AU,1]\n",
    "        au_per_video = np.array(au_per_video, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        if isinstance(self.symmetry_dir, list):\n",
    "            symmetry_file = os.path.join(self.symmetry_dir[idx],  self.symmetry_file_names[idx])\n",
    "        else:\n",
    "            symmetry_file = os.path.join(self.symmetry_dir,  self.symmetry_file_names[idx])\n",
    "        \n",
    "        symmetry_df = pd.read_csv(symmetry_file,header=None,index_col=False)\n",
    "        symmetry_array = symmetry_df.values\n",
    "        symmetry_array = symmetry_array[0]\n",
    "        symmetry_per_video = np.array(symmetry_array, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        if isinstance(self.gaze_dir, list):\n",
    "            gaze_file = os.path.join(self.gaze_dir[idx],  self.gaze_file_names[idx])\n",
    "        else:\n",
    "            gaze_file = os.path.join(self.gaze_dir,  self.gaze_file_names[idx])\n",
    "        \n",
    "        gaze_df = pd.read_csv(gaze_file,header=None,index_col=False)\n",
    "        gaze_array = gaze_df.values\n",
    "        gaze_array = gaze_array[1:,1:]\n",
    "        gaze_array = np.reshape(gaze_array,(36))\n",
    "        gaze_per_video = np.array(gaze_array, dtype=np.float32)\n",
    "        \n",
    "        if isinstance(self.ME_dir, list):\n",
    "            ME_file = os.path.join(self.ME_dir[idx],  self.ME_file_names[idx])\n",
    "        else:\n",
    "            ME_file = os.path.join(self.ME_dir,  self.ME_file_names[idx])\n",
    "        \n",
    "        ME_df = pd.read_csv(ME_file,header=None,index_col=False)\n",
    "        ME_array = ME_df.values\n",
    "        ME_array = ME_array[1:,1]\n",
    "        ME_per_video = np.array(ME_array, dtype=np.float32)\n",
    "        \n",
    "        au_per_video = np.concatenate((au_per_video, symmetry_per_video), axis=0)\n",
    "        au_per_video = np.concatenate((au_per_video, gaze_per_video), axis=0)\n",
    "        au_per_video = np.concatenate((au_per_video, ME_per_video), axis=0)\n",
    "        \n",
    "       \n",
    "        sample = {'feature': feature_per_video, 'label': label_per_video, 'AUs': au_per_video}\n",
    "        \n",
    "        \n",
    "        return sample #, video_names\n",
    "\n",
    "\n",
    "############### Select Input ###################\n",
    "data_name = 'Real life trial dataset'\n",
    "data_path_dir_test, label_dict_test, video_name_list_test, au_path_test, au_file_list_test, symmetry_path_test, symmetry_file_list_test, gaze_path_test, gaze_file_list_test, ME_path_test, ME_file_list_test = data_to_dict_test(data_name)\n",
    "data_path_dir_train, label_dict_train, video_name_list_train, au_path_train, au_file_list_train, symmetry_path_train, symmetry_file_list_train, gaze_path_train, gaze_file_list_train, ME_path_train, ME_file_list_train = data_to_dict_train(data_name)\n",
    "################################################\n",
    "\n",
    "############## Load Data ######################\n",
    "train_dataset = DeceptionDB(label_dict_train, data_path_dir_train, video_name_list_train, au_path_train, au_file_list_train, symmetry_path_train, symmetry_file_list_train, gaze_path_train, gaze_file_list_train, ME_path_train, ME_file_list_train)\n",
    "test_dataset = DeceptionDB(label_dict_test, data_path_dir_test, video_name_list_test, au_path_test, au_file_list_test, symmetry_path_test, symmetry_file_list_test, gaze_path_test, gaze_file_list_test, ME_path_test, ME_file_list_test)\n",
    "train_total_data_num = len(video_name_list_train)\n",
    "test_total_data_num = len(video_name_list_test)\n",
    "################################################\n",
    "\n",
    "############### learning Parameters ###############\n",
    "init_batch_size = 12 # or 6\n",
    "max_epoch = 30 # or 60\n",
    "num_segments = frame_len # video frames ****** Check ******\n",
    "use_regularizer = True\n",
    "hp_reg_factor = 1 # or 1\n",
    "tv_reg_factor = 1e-5 # or 0.005\n",
    "constrast_reg_factor = 1e-5 # or 1\n",
    "init_lr = 0.005 # 1e-5\n",
    "weight_decay = 1e-5  # or 1e-5\n",
    "#lr_patience = 5 # or 3\n",
    "dropout_ratio = 0.8 # or 0.4\n",
    "class_num = 2\n",
    "hidden = 2 #14\n",
    "###################################################\n",
    "\n",
    "\n",
    "class Action_Att_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, seq_len):\n",
    "        super(Action_Att_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc_attention = nn.Linear(hidden_size, 1)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.fc_c0_0 = nn.Linear(1792, 896)\n",
    "        self.fc_c0_1 = nn.Linear(896, 448)\n",
    "        self.fc_h0_0 = nn.Linear(1792,896)\n",
    "        self.fc_h0_1 = nn.Linear(896, 448)\n",
    "        self.input_size = input_size\n",
    "        self.fc_au_0 = nn.Linear(58, hidden_size) #### 4 AU durations + 1 symmetry correlation + 36 gaze_features + 17 Micro Expression\n",
    "        self.fc_au_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_fusion_out_0 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_fusion_out_1 = nn.Linear(hidden_size, output_size)\n",
    "        # Not all fc layer used\n",
    "\n",
    "        self.mask_conv = nn.Sequential(\n",
    "                nn.Conv2d(1792, 896, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(896),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(896, 448, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(448),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(448, 1, kernel_size=3, padding=1, bias=False),\n",
    "                nn.Sigmoid(), #(bs*22, 1, 8, 8)\n",
    "                )       \n",
    "        self.batchnorm_2d = nn.BatchNorm2d(1792)\n",
    "        self.batchnorm_3d_1 = nn.BatchNorm3d(num_segments)\n",
    "        self.batchnorm_3d_2 = nn.BatchNorm3d(num_segments)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.dropout_2d = nn.Dropout2d(p=dropout_ratio)\n",
    "        self.dropout_3d = nn.Dropout3d(p=dropout_ratio)\n",
    "        self.conv_lstm = ConvLSTM(input_size=(3, 3),\n",
    "                input_dim=1792,\n",
    "                hidden_dim=[hidden],\n",
    "                kernel_size=(3, 3),\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bias=True,\n",
    "                return_all_layers=True)      \n",
    "\n",
    "    def forward(self, input_x, AU):\n",
    "\n",
    "        batch_size = input_x.shape[0] \n",
    "        seq_len = input_x.shape[1]\n",
    "        input_x = self.dropout_2d(input_x)\n",
    "        input_x = input_x.view(-1, 1792, 3, 3) # num_segments*batch_size\n",
    "        input_x = self.batchnorm_2d(input_x)\n",
    "        \n",
    "        mask = self.mask_conv(input_x)\n",
    "        mask = mask.view(-1, num_segments, 1, 3, 3) #  batch_size\n",
    "        input_x = input_x.view(-1, num_segments, 1792, 3, 3) # batch_size\n",
    "        input_x = self.batchnorm_3d_1(input_x)\n",
    "        diff_i = torch.sum(torch.abs(mask[:, :, :, :, 1:] - mask[:, :, :, :, :-1]))\n",
    "        diff_j = torch.sum(torch.abs(mask[:, :, :, 1:, :] - mask[:, :, :, :-1, :]))\n",
    "        tv_loss = tv_reg_factor*(diff_i + diff_j)\n",
    "        mask_A = (mask > 0.5).type( torch.cuda.FloatTensor )\n",
    "        mask_B = (mask < 0.5).type( torch.cuda.FloatTensor )\n",
    "        contrast_loss = -(mask * mask_A).mean(0).sum() * constrast_reg_factor* 0.5 + (mask * mask_B).mean(0).sum() * constrast_reg_factor * 0.5\n",
    "\n",
    "        mask_input_x = mask * input_x\n",
    "        mask_input_x = self.batchnorm_3d_2(mask_input_x)\n",
    "        \n",
    "        del input_x\n",
    "        output, hidden = self.conv_lstm(mask_input_x)\n",
    "        del hidden\n",
    "        del mask_input_x\n",
    "        output = output[0]\n",
    "        output = torch.mean(output,dim=4)\n",
    "        output = torch.mean(output,dim=3)\n",
    "        att_weight = self.fc_attention(output).view(-1, num_segments)\n",
    "        att_weight = F.softmax(att_weight, dim =1)     \n",
    "        weighted_output = torch.sum(output*att_weight.unsqueeze(dim=2),\n",
    "                                        dim =1)\n",
    "        \n",
    "        au_feature_tmp = self.fc_au_0(AU)\n",
    "        au_feature = self.fc_au_1(au_feature_tmp)\n",
    "        fusion_input = torch.cat([weighted_output, au_feature], dim=1)\n",
    "        semi_final_output = self.fc_fusion_out_0(fusion_input)\n",
    "        final_output = self.fc_fusion_out_1(semi_final_output)\n",
    "        \n",
    "        del weighted_output\n",
    "        del diff_i\n",
    "        del diff_j\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return final_output, att_weight, mask, tv_loss, contrast_loss\n",
    " \n",
    "    def init_hidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "def train(batch_size,\n",
    "        train_data,\n",
    "        train_label,\n",
    "        model,\n",
    "        model_optimizer,\n",
    "        criterion,\n",
    "         train_au):\n",
    "    loss = 0\n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    logits, att_weight, mask, tv_loss, contrast_loss = model.forward(train_data,train_au)\n",
    "    del train_data\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    loss += criterion(logits, torch.max(train_label,1)[1])\n",
    "    att_reg = F.relu(att_weight[:, :-2] * att_weight[:, 2:] - att_weight[:, 1:-1].pow(2)).sqrt().mean()\n",
    "    \n",
    "    if use_regularizer:\n",
    "        regularization_loss = hp_reg_factor*att_reg \n",
    "        loss += regularization_loss\n",
    "        loss += tv_loss\n",
    "        loss += contrast_loss\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    model_optimizer.step()\n",
    "\n",
    "    final_loss = loss.item()\n",
    "    corrects = []\n",
    "    correct = (torch.max(logits, 1)[1] == torch.max(train_label,1)[1])\n",
    "    corrects.append(correct)\n",
    "    corrects_sum = (torch.max(logits, 1)[1] == torch.max(train_label,1)[1]).sum()\n",
    "\n",
    "    train_accuracy = 100.0 * corrects_sum/batch_size\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return mask, final_loss, regularization_loss, tv_loss, contrast_loss, train_accuracy, att_weight, corrects, logits, train_label\n",
    "\n",
    "def test_step(batch_size,\n",
    "            batch_x,\n",
    "            batch_y,\n",
    "            model,\n",
    "            criterion,\n",
    "              test_au):\n",
    "    test_loss=0\n",
    "    test_logits, att_weight, mask, tv_loss, contrast_loss = model.forward(batch_x,test_au)\n",
    "    del batch_x\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    test_loss += criterion(test_logits, torch.max(batch_y,1)[1])\n",
    "    att_reg = F.relu(att_weight[:, :-2] * att_weight[:, 2:] - att_weight[:, 1:-1].pow(2)).sqrt().mean()\n",
    "    \n",
    "    if use_regularizer:\n",
    "        test_reg_loss = hp_reg_factor*att_reg \n",
    "        test_loss += test_reg_loss\n",
    "        test_loss += tv_loss\n",
    "        test_loss += contrast_loss\n",
    "            \n",
    "    corrects = []\n",
    "    correct = (torch.max(test_logits, 1)[1] == torch.max(batch_y,1)[1])\n",
    "    corrects.append(correct)\n",
    "    corrects_sum = (torch.max(test_logits, 1)[1] == torch.max(batch_y,1)[1]).sum()\n",
    "    del correct\n",
    "    test_accuracy = 100.0 * corrects_sum/batch_size\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return mask, test_logits, test_loss, test_reg_loss, tv_loss, contrast_loss, test_accuracy, att_weight, corrects, test_logits, batch_y\n",
    "\n",
    "################ main Start ! ##################\n",
    "\n",
    "torch.cuda.manual_seed(1234)\n",
    "maxEpoch = max_epoch\n",
    "num_segments = num_segments\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "best_test_accuracy = 0\n",
    "lstm_action = Action_Att_LSTM(input_size=1792, hidden_size=hidden, output_size=2, seq_len=num_segments).cuda()\n",
    "model_optimizer = torch.optim.Adam(lstm_action.parameters(), lr=init_lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=model_optimizer, lr_lambda= lambda epoch: 0.99 ** epoch)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=init_batch_size)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=init_batch_size)\n",
    "num_step_per_epoch_train = train_total_data_num/init_batch_size\n",
    "num_step_per_epoch_test = test_total_data_num/init_batch_size\n",
    "\n",
    "\n",
    "for epoch_num in range(maxEpoch):\n",
    "    lstm_action.train() # ex) test(input.to(device)) behind input\n",
    "    avg_train_accuracy = 0\n",
    "    train_name_list = video_name_list_train\n",
    "    epoch_train_loss = 0 \n",
    "    epoch_train_reg_loss = 0 \n",
    "    epoch_train_tv_loss = 0\n",
    "    epoch_train_contrast_loss = 0\n",
    "    for i, train_sample in enumerate(train_data_loader):\n",
    "        train_batch_feature = train_sample['feature']\n",
    "        train_batch_label = train_sample['label']\n",
    "        train_batch_au = train_sample['AUs']\n",
    "        train_batch_label = train_batch_label.reshape(-1)\n",
    "        one_hot_train_batch_label = np.eye(2)[train_batch_label] # 2 means num_class !\n",
    "        one_hot_train_batch_label = torch.from_numpy(one_hot_train_batch_label)\n",
    "        train_batch_feature = Variable(train_batch_feature).cuda().float()\n",
    "        train_batch_label = Variable(one_hot_train_batch_label).cuda().long()\n",
    "        train_batch_au = Variable(train_batch_au).cuda().float()\n",
    "        train_mask, train_loss, train_reg_loss, train_tv_loss, train_contrast_loss, train_accuracy, train_tmp_att_weights, train_corrects, train_pred, train_label = train(init_batch_size, train_batch_feature, train_batch_label, lstm_action, model_optimizer, criterion, train_batch_au)\n",
    "        avg_train_accuracy+=train_accuracy\n",
    "        epoch_train_loss += train_loss\n",
    "        epoch_train_reg_loss += train_reg_loss\n",
    "        epoch_train_tv_loss += train_tv_loss\n",
    "        epoch_train_contrast_loss += train_contrast_loss\n",
    "        #print(\"batch {}, train_loss: {} \".format(i, train_loss))\n",
    "        #print(\"batch {}, train_acc: {} \".format(i, train_accuracy))\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_train_loss = epoch_train_loss/num_step_per_epoch_train\n",
    "    epoch_train_reg_loss = epoch_train_reg_loss/num_step_per_epoch_train\n",
    "    epoch_train_tv_loss = epoch_train_tv_loss/num_step_per_epoch_train\n",
    "    epoch_train_contrast_loss = epoch_train_contrast_loss/num_step_per_epoch_train\n",
    "    final_train_accuracy = avg_train_accuracy/num_step_per_epoch_train\n",
    "    print(\"epoch:{} \".format(epoch_num)+ \" train loss: {}\".format(epoch_train_loss))\n",
    "    print(\"epoch:{} \".format(epoch_num)+ \" train accuracy: {}\".format(final_train_accuracy))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_test_accuracy = 0\n",
    "    lstm_action.eval()\n",
    "    test_name_list = video_name_list_test\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_reg_loss =0\n",
    "    epoch_test_tv_loss =0 \n",
    "    epoch_test_contrast_loss = 0\n",
    "    for i, test_sample in enumerate(test_data_loader):\n",
    "        test_batch_feature = test_sample['feature']\n",
    "        test_batch_label = test_sample['label']\n",
    "        test_batch_au = test_sample['AUs']\n",
    "        test_batch_label = test_batch_label.reshape(-1)\n",
    "        one_hot_test_batch_label = np.eye(2)[test_batch_label] # 2 means num_class !\n",
    "        one_hot_test_batch_label = torch.from_numpy(one_hot_test_batch_label)\n",
    "        with torch.no_grad():\n",
    "            test_batch_feature = Variable(test_batch_feature).cuda().float()\n",
    "            test_batch_label = Variable(one_hot_test_batch_label).cuda().long()\n",
    "            test_batch_au = Variable(test_batch_au).cuda().float()\n",
    "            test_mask, test_logits, test_loss, test_reg_loss, test_tv_loss, test_contrast_loss, test_accuracy, test_tmp_att_weights, test_corrects, test_pred, test_label = test_step(init_batch_size, test_batch_feature, test_batch_label, lstm_action, criterion, test_batch_au)\n",
    "\n",
    "        #print(\"batch_{}, test_loss: {}\".format(i, test_loss))\n",
    "        #print(\"batch_{}, test_accuracy: {}\".format(i, test_accuracy))\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        avg_test_accuracy+= test_accuracy\n",
    "        epoch_test_loss += test_los\n",
    "        epoch_test_reg_loss += test_reg_loss\n",
    "        epoch_test_tv_loss += test_tv_loss\n",
    "        epoch_test_contrast_loss += test_contrast_loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    epoch_test_loss = epoch_test_loss/num_step_per_epoch_test\n",
    "    epoch_test_reg_loss = epoch_test_reg_loss/num_step_per_epoch_test\n",
    "    epoch_test_tv_loss = epoch_test_tv_loss/num_step_per_epoch_test\n",
    "    epoch_test_contrast_loss = epoch_test_contrast_loss/num_step_per_epoch_test\n",
    "\n",
    "    final_test_accuracy = avg_test_accuracy/num_step_per_epoch_test\n",
    "    print(\"epoch: {} \".format(epoch_num)+ \" test loss:{} \".format(epoch_test_loss))\n",
    "    print(\"epoch: {} \".format(epoch_num)+ \" test accuracy:{} \".format(final_test_accuracy))\n",
    "                                             \n",
    "    model_optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(\"learning rate is : {}\".format(model_optimizer.param_groups[0]['lr']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
